{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I add learning embeddings with normalization. The normalization method is taken from [A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings](https://arxiv.org/abs/1805.06297) by Mikel Artetxe, Gorka Labaka and Eneko Agirre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PermuteEmbeddingNorm(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        # to conform to nn.Embedding api\n",
    "        self.max_norm=None\n",
    "        self.norm_type=2.0\n",
    "        self.scale_grad_by_freq=False\n",
    "        self.sparse = False\n",
    "\n",
    "        self.weight = nn.Parameter( torch.Tensor(num_embeddings, embedding_dim) )\n",
    "        nn.init.kaiming_uniform_(self.weight)\n",
    "        self.p = nn.Parameter( torch.eye(self.num_embeddings) )\n",
    "        self.p.requires_grad = False\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, words):\n",
    "        return F.embedding(words, self.p @ self.normalized_weight())\n",
    "    \n",
    "    def normalized_weight(self):\n",
    "        w1 = self.weight / self.weight.norm(dim=1).unsqueeze(1)\n",
    "        w2 = w1 - w1.mean(0)\n",
    "        w3 = w2 / w2.norm(dim=1).unsqueeze(1)\n",
    "        return w3\n",
    "        \n",
    "    def reset_parameters(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a new architecture that will be using our new encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class p_normAWD_LSTM(AWD_LSTM):\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
    "                 weight_p=0.5, bidir=False, packed=False):\n",
    "        store_attr(self, 'emb_sz,n_hid,n_layers,pad_token,packed')\n",
    "        self.bs = 1\n",
    "        self.n_dir = 2 if bidir else 1\n",
    "        self.encoder = PermuteEmbeddingNorm(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = self.encoder \n",
    "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
    "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeated the experiments as done up to this point with normalization. There has been no significant improvement on the translation task.\n",
    "\n",
    "The next thing to try is aligning the embeddings using [vecmap](https://github.com/artetxem/vecmap). The results can be telling when it comes to understanding the quality of our embeddings and the evaluation framework. Are our embeddings lending themselves at all to the translation task? (maybe the vocab is too small, maybe they are not of high enough quality). Is the evaluation framework using the English to Spanish dictionary working at all? Maybe there is a bug preventing it from doing anything useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_train_LM_en.ipynb.\n",
      "Converted 02_train_LM_es.ipynb.\n",
      "Converted 03_translate_en_to_es.ipynb.\n",
      "Converted 04_LM_with_normalized_embeddings.ipynb.\n",
      "Converted 05_aligning_the_embeddings_using_vecmap.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
