{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_head.core import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermuteEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        # to conform to nn.Embedding api\n",
    "        self.max_norm=None\n",
    "        self.norm_type=2.0\n",
    "        self.scale_grad_by_freq=False\n",
    "        self.sparse = False\n",
    "\n",
    "        self.weight = nn.Parameter( torch.Tensor(num_embeddings, embedding_dim) )\n",
    "        nn.init.kaiming_uniform_(self.weight)\n",
    "        self.p = nn.Parameter( torch.eye(self.num_embeddings) )\n",
    "        self.p.requires_grad = False\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, words):\n",
    "        return F.embedding(words, self.p @ self.normalized_weight())\n",
    "    \n",
    "    def normalized_weight(self):\n",
    "        w1 = self.weight / self.weight.norm(dim=1).unsqueeze(1)\n",
    "        w2 = w1 - w1.mean(0)\n",
    "        w3 = w2 / w2.norm(dim=1).unsqueeze(1)\n",
    "        return w3\n",
    "        \n",
    "    def reset_parameters(self): pass\n",
    "\n",
    "import decoder_head.core\n",
    "decoder_head.core.PermuteEmbedding = PermuteEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en= make_vocab(pd.read_pickle('data/en-100_tok/counter.pkl'), max_vocab=4000)\n",
    "vocab_es= make_vocab(pd.read_pickle('data/es-100_tok/counter.pkl'), max_vocab=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/en-100_tok/'\n",
    "mult = 4\n",
    "bs = 80\n",
    "seq_len = 70\n",
    "\n",
    "lm = DataBlock(blocks=(TextBlock(vocab=vocab_en, is_lm=True),),\n",
    "                get_x=read_tokenized_file,\n",
    "                get_items=partial(get_text_files, folders=['train', 'valid']),\n",
    "                splitter=FuncSplitter(lambda itm: itm.parent.name == 'valid'))\n",
    "\n",
    "dbunch_lm = lm.databunch(path, path=path, bs=bs, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dbunch_lm,\n",
    "    pAWD_LSTM,\n",
    "    opt_func=opt,\n",
    "    pretrained=False,\n",
    "    config=awd_lstm_lm_config,\n",
    "    drop_mult=0.1,\n",
    "    metrics=[accuracy, top_k_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.text.learner.LMLearner at 0x7fc44d5fa2d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('normalized_mapped_en') # en LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en-es.txt') as f:\n",
    "    en_es = f.readlines()\n",
    "    \n",
    "en_es = [l.strip() for l in en_es]\n",
    "\n",
    "en_es_dict = defaultdict(list)\n",
    "\n",
    "for l in en_es:\n",
    "    source, target = l.split()\n",
    "    en_es_dict[source].append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_es_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have the source word in the model trained on English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en_set = set(vocab_en)\n",
    "en_es_dict = {k: v for k, v in en_es_dict.items() if k in vocab_en_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_es_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have the target word in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_es_set = set(vocab_es)\n",
    "en_es_dict = {k: vocab_es_set.intersection(set(v)) for k, v in en_es_dict.items() if vocab_es_set.intersection(set(v))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2451"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_es_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attemp translation from English to Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.optimize_permutation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(learn.model[0].encoder.p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_embs(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    vocab = []\n",
    "    embs = []\n",
    "    for line in lines[1:]:\n",
    "        l = line.split()\n",
    "        vocab.append(l[0])\n",
    "        embs.append(np.array([float(s) for s in l[1:]]))\n",
    "    return vocab, np.stack(embs)\n",
    "\n",
    "vocab_embs, embs = txt_to_embs('data/es_norm_embs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model[0].encoder.weight = nn.Parameter(tensor(embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].encoder.p.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model[0].encoder.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an English LM with Spanish embeddings loaded. Let's see how we do on translation.\n",
    "\n",
    "One thing worth keeping in mind are the missing English words from the Spanish vocabulary. This is an interesting situation and makes the task even harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = lambda preds, targs: aza_loss(learn, preds, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     train_loss  valid_loss  accuracy  top_k_accuracy  time    \n",
      "0         3.684969    3.639772    0.344198  0.591861        40:21     \n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3,  moms=(0.8, 0.7, 0.8), wd=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('pLSTM_emb_norm_en-es_hinted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.text.learner.LMLearner at 0x7fc44d5fa2d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('pLSTM_emb_norm_en-es_hinted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#3) [3.639772415161133,0.3441983759403229,0.5918610692024231]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_translation_acc(top_n=1):\n",
    "    p = learn.model[0].encoder.p\n",
    "    hits = 0\n",
    "    for k, v in en_es_dict.items():\n",
    "        idx_en = vocab_en.index(k)\n",
    "        for vv in v:\n",
    "            idx_es = vocab_es.index(vv)\n",
    "            if idx_es in p[idx_en].argsort(descending=True)[:top_n]:\n",
    "                hits += 1\n",
    "#                 print(k, vv)\n",
    "                break\n",
    "    return hits/len(en_es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041207670338637294"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_acc(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031007751937984496"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_acc(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_translation_acc(top_n=1):\n",
    "    p = learn.model[0].encoder.p\n",
    "    hits = 0\n",
    "    for k, v in en_es_dict.items():\n",
    "        idx_en = vocab_en.index(k)\n",
    "        for vv in v:\n",
    "            idx_es = vocab_es.index(vv)\n",
    "            if idx_es in p[idx_en].argsort(descending=True)[:top_n]:\n",
    "                hits += 1\n",
    "                print(k, [vocab_es[i] for i in p[idx_en].argsort(descending=True)[:5]])\n",
    "                break\n",
    "    return hits/len(en_es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'han', 'tener', 'tienen'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_es_dict['have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should ['varía', 'perdió', 'limita', 'finalizó', 'debería']\n",
      "authority ['jurisdicción', 'honor', 'river', 'autoridad', 'autonomía']\n",
      "oklahoma ['empleados', 'medallas', 'oklahoma', 'productores', 'us']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0012239902080783353"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_acc(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17130944, -0.13466078,  0.15399924, ..., -0.19588749,\n",
       "        -0.08263521,  0.18715824],\n",
       "       [ 0.10057934,  0.04197982, -0.10051062, ...,  0.11448281,\n",
       "         0.08857403, -0.11117812],\n",
       "       [ 0.04381532,  0.09861507, -0.04373881, ...,  0.05031699,\n",
       "         0.12407271, -0.00830902],\n",
       "       ...,\n",
       "       [ 0.10647476,  0.00848802, -0.07712181, ...,  0.10510191,\n",
       "        -0.01276942, -0.09851178],\n",
       "       [ 0.16311131,  0.08484522, -0.12757055, ...,  0.15140425,\n",
       "         0.05144875, -0.04720284],\n",
       "       [ 0.0999221 ,  0.06762829, -0.1189317 , ...,  0.13498293,\n",
       "         0.08796506, -0.10488362]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1713, -0.1347,  0.1540,  ..., -0.1959, -0.0826,  0.1872],\n",
       "        [ 0.1006,  0.0420, -0.1005,  ...,  0.1145,  0.0886, -0.1112],\n",
       "        [ 0.0438,  0.0986, -0.0437,  ...,  0.0503,  0.1241, -0.0083],\n",
       "        ...,\n",
       "        [ 0.1065,  0.0085, -0.0771,  ...,  0.1051, -0.0128, -0.0985],\n",
       "        [ 0.1631,  0.0848, -0.1276,  ...,  0.1514,  0.0514, -0.0472],\n",
       "        [ 0.0999,  0.0676, -0.1189,  ...,  0.1350,  0.0880, -0.1049]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].encoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3910e-10,  1.0744e-09, -6.4294e-09,  ..., -5.4824e-09,\n",
       "        -9.8421e-09, -3.4995e-09], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].encoder.p[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_translation_acc(top_n=1):\n",
    "    p = learn.model[0].encoder.p\n",
    "    hits = 0\n",
    "    for k, v in en_es_dict.items():\n",
    "        idx_en = vocab_en.index(k)\n",
    "        print(k, [vocab_es[i] for i in p[idx_en].argsort(descending=True)[:5]])\n",
    "        for vv in v:\n",
    "            idx_es = vocab_es.index(vv)\n",
    "            if idx_es in p[idx_en].argsort(descending=True)[:top_n]:\n",
    "                hits += 1\n",
    "                break\n",
    "    return hits/len(en_es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should ['varía', 'perdió', 'limita', 'finalizó', 'debería']\n",
      "authority ['jurisdicción', 'honor', 'river', 'autoridad', 'autonomía']\n",
      "oklahoma ['empleados', 'medallas', 'oklahoma', 'productores', 'us']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0012239902080783353"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_acc(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_translation_acc(top_n=1):\n",
    "    p = learn.model[0].encoder.p\n",
    "    learned_embeddings = learn.model[0].encoder.p @ learn.model[0].encoder.weight\n",
    "    \n",
    "    hits = 0\n",
    "    for k, v in en_es_dict.items():\n",
    "        idx_en = vocab_en.index(k)\n",
    "        diff = learn.model[0].encoder.weight - learned_embeddings[idx_en]\n",
    "        candidates = diff.sum(-1).argsort(descending=False)[:top_n]\n",
    "        \n",
    "        for vv in v:\n",
    "            idx_es = vocab_es.index(vv)\n",
    "            if idx_es in candidates:\n",
    "                hits += 1\n",
    "                break\n",
    "    return hits/len(en_es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03386372909016728"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_acc(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_translation_acc(top_n=1):\n",
    "    p = learn.model[0].encoder.p\n",
    "    learned_embeddings = learn.model[0].encoder.p @ learn.model[0].encoder.normalized_weight()\n",
    "    \n",
    "    hits = 0\n",
    "    for k, v in en_es_dict.items():\n",
    "        idx_en = vocab_en.index(k)\n",
    "        diff = learn.model[0].encoder.normalized_weight() - learned_embeddings[idx_en]\n",
    "        candidates = diff.sum(-1).argsort(descending=False)[:top_n]\n",
    "        \n",
    "        for vv in v:\n",
    "            idx_es = vocab_es.index(vv)\n",
    "            if idx_es in candidates:\n",
    "                hits += 1\n",
    "                break\n",
    "    return hits/len(en_es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03223174214606283"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_acc(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
