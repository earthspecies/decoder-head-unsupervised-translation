{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, we will use data gratiously shared by authors of [MultiFiT: Efficient Multi-lingual Language Model Fine-tuning](MultiFiT: Efficient Multi-lingual Language Model Fine-tuning). Here is the accompanying [repository](https://github.com/n-waves/multifit).\n",
    "\n",
    "We are interested in the wikipedia dumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-15 15:38:36--  https://www.dropbox.com/sh/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.1, 2620:100:6022:1::a27d:4201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /sh/dl/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki [following]\n",
      "--2020-01-15 15:38:36--  https://www.dropbox.com/sh/dl/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucc2173abafec6a55ac59cf336c1.dl.dropboxusercontent.com/zip_download_get/ATnkaMkel-an1mWzaVuIlopUYOOy0nk5B1YMPr9O1l4CSWePQ1KiiVrW15DtzbgEWZCkMikT41F2FnRHnusM8iMTLKivRPxdPjsLgxv5QcJv9w?dl=1 [following]\n",
      "--2020-01-15 15:38:37--  https://ucc2173abafec6a55ac59cf336c1.dl.dropboxusercontent.com/zip_download_get/ATnkaMkel-an1mWzaVuIlopUYOOy0nk5B1YMPr9O1l4CSWePQ1KiiVrW15DtzbgEWZCkMikT41F2FnRHnusM8iMTLKivRPxdPjsLgxv5QcJv9w?dl=1\n",
      "Resolving ucc2173abafec6a55ac59cf336c1.dl.dropboxusercontent.com (ucc2173abafec6a55ac59cf336c1.dl.dropboxusercontent.com)... 162.125.66.6, 2620:100:6022:6::a27d:4206\n",
      "Connecting to ucc2173abafec6a55ac59cf336c1.dl.dropboxusercontent.com (ucc2173abafec6a55ac59cf336c1.dl.dropboxusercontent.com)|162.125.66.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1840410681 (1.7G) [application/zip]\n",
      "Saving to: ‘data/preprocessed_wiki_8langs.zip’\n",
      "\n",
      "data/preprocessed_w 100%[===================>]   1.71G  5.39MB/s    in 6m 0s   \n",
      "\n",
      "2020-01-15 15:44:39 (4.88 MB/s) - ‘data/preprocessed_wiki_8langs.zip’ saved [1840410681/1840410681]\n",
      "\n",
      "Archive:  data/preprocessed_wiki_8langs.zip\n",
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n",
      " extracting: data/README             \n",
      " extracting: data/fr-100.tar.gz      \n",
      " extracting: data/en-100.tar.gz      \n",
      " extracting: data/it-100.tar.gz      \n",
      " extracting: data/es-100.tar.gz      \n",
      " extracting: data/ja-100.tar.gz      \n",
      " extracting: data/pl-100.tar.gz      \n",
      " extracting: data/ru-100.tar.gz      \n",
      " extracting: data/de-100.tar.gz      \n",
      " extracting: data/zh-100.tar.gz      \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget -nc 'https://www.dropbox.com/sh/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki?dl=1' -O 'data/preprocessed_wiki_8langs.zip'\n",
    "!unzip 'data/preprocessed_wiki_8langs.zip' -d 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the English and Spanish wiki dumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-100/\n",
      "en-100/en.wiki.valid.tokens\n",
      "en-100/en.wiki.train.tokens\n",
      "en-100/en.wiki.test.tokens\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf data/en-100.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es-100/\n",
      "es-100/es.wiki.test.tokens\n",
      "es-100/es.wiki.train.tokens\n",
      "es-100/es.wiki.valid.tokens\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf data/es-100.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \r\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \r\n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \r\n",
      " \r\n",
      " = = Gameplay = = \r\n",
      " \r\n",
      " As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \r\n"
     ]
    }
   ],
   "source": [
    "!head data/en-100/en.wiki.train.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es-100/\n",
      "es-100/es.wiki.test.tokens\n",
      "es-100/es.wiki.train.tokens\n",
      "es-100/es.wiki.valid.tokens\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf data/es-100.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mde-100.tar.gz\u001b[0m*  \u001b[01;32mes-100.tar.gz\u001b[0m*  \u001b[01;32mja-100.tar.gz\u001b[0m*                \u001b[01;32mREADME\u001b[0m*\r\n",
      "\u001b[01;32men-100.tar.gz\u001b[0m*  es-clean.txt    \u001b[01;32mpl-100.tar.gz\u001b[0m*                \u001b[01;32mru-100.tar.gz\u001b[0m*\r\n",
      "en-clean.txt    \u001b[01;32mfr-100.tar.gz\u001b[0m*  pl.txt                        \u001b[01;32mzh-100.tar.gz\u001b[0m*\r\n",
      "en.txt          \u001b[01;32mit-100.tar.gz\u001b[0m*  \u001b[01;31mpreprocessed_wiki_8langs.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mde-100.tar.gz\u001b[0m*  \u001b[01;32mes-100.tar.gz\u001b[0m*  \u001b[01;32mja-100.tar.gz\u001b[0m*                \u001b[01;32mREADME\u001b[0m*\r\n",
      "\u001b[01;32men-100.tar.gz\u001b[0m*  es-clean.txt    \u001b[01;32mpl-100.tar.gz\u001b[0m*                \u001b[01;32mru-100.tar.gz\u001b[0m*\r\n",
      "en-clean.txt    \u001b[01;32mfr-100.tar.gz\u001b[0m*  pl.txt                        \u001b[01;32mzh-100.tar.gz\u001b[0m*\r\n",
      "en.txt          \u001b[01;32mit-100.tar.gz\u001b[0m*  \u001b[01;31mpreprocessed_wiki_8langs.zip\u001b[0m\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with text. [Project Gutenberg](https://www.gutenberg.org/wiki/Main_Page) is a great source for obtaining books that are in the public domain!\n",
    "\n",
    "Let's put together some rudimentary mechanism for obtaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import urllib.request\n",
    "from fastcore.foundation import *\n",
    "import re\n",
    "\n",
    "def download_ebook(url, skip_lines=0, drop_lines=None):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read()\n",
    "    text = data.decode('utf-8')\n",
    "    lst =  L(text.split('\\r\\n'))\n",
    "    lst = lst.filter(lambda x: x != '').map(lambda x: x.strip()).map(lambda x: re.sub(r'(—|_|“|”)', '', x))\n",
    "    if drop_lines is not None: drop_lines = -drop_lines\n",
    "    return ' '.join(lst[skip_lines:drop_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_prejudice_url = 'https://www.gutenberg.org/files/1342/1342-0.txt'\n",
    "text = download_ebook(pride_and_prejudice_url, 82, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. My dear Mr. Bennet, said his lady to him one day, have you heard that Netherfield Park is let at last? Mr. Bennet replied th'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this functionality to create a couple of small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ''.join(\n",
    "    [\n",
    "        download_ebook('https://www.gutenberg.org/files/1342/1342-0.txt', 82, 300), # Pride and Prejudice\n",
    "        download_ebook('https://www.gutenberg.org/files/4300/4300-0.txt', 100, 340), # Ulysses\n",
    "        download_ebook('https://www.gutenberg.org/files/16/16-0.txt', 100, 340) # Peter Pan\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431933"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/en.txt\", \"w\") as text_file:\n",
    "    text_file.write(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there are not that many books in Polish on Project Gutenberg, however there is another site, wolnelektury.pl with a lot of books in the public domain.\n",
    "\n",
    "The focus of this repository will be unsupervised translation and working with English and Polish will be easiest for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = ''.join(\n",
    "    [\n",
    "        download_ebook('https://wolnelektury.pl/media/book/txt/robinson-crusoe.txt', 100, 300), # Robinson Crusoe\n",
    "        download_ebook('https://wolnelektury.pl/media/book/txt/lord-jim-tom-pierwszy.txt', 100, 300), # Lord Jim\n",
    "        download_ebook('https://wolnelektury.pl/media/book/txt/kipling-druga-ksiega-dzungli.txt', 100, 300) # Second Jungle Book\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105637"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pl.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pl.txt\", \"w\") as text_file:\n",
    "    text_file.write(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 99_index.ipynb.\n",
      "Converted language_play_v9.ipynb.\n",
      "Converted language_play_v9_softmax.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
